{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "286bc17f",
   "metadata": {},
   "source": [
    "# Meta-Scale ML Recommendation & Observability System\n",
    "**Architectural Blueprint for Real-time Personalized Recommendations**\n",
    "\n",
    "This notebook serves as a technical design document for a high-performance recommendation engine. It covers the end-to-end lifecycle from raw data ingestion to \"Self-Healing\" production monitoring.\n",
    "\n",
    "### **Some Interesting Reads:**  \n",
    "### - [Two-Tower Model Deep Dive (shaped.ai)](https://www.shaped.ai/blog/the-two-tower-model-for-recommendation-systems-a-deep-dive) • [MLOps Pipelines (Google Cloud)](https://docs.cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning) • [Michelangelo ML Platform (Uber)](https://www.uber.com/en-IN/blog/michelangelo-machine-learning-platform/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0ad15d",
   "metadata": {},
   "source": [
    "## 1. High Level Architecture\n",
    "\n",
    "### Key Components:\n",
    "* **Layer 1 (Ingestion):** A Lambda architecture handling 100k+ events/sec via Kafka (Stream) and S3 (Batch).\n",
    "* **Layer 2 (Feature Store):** Decouples state management to solve the Training-Serving skew.\n",
    "* **Layer 3 (Inference):** A multi-stage funnel (Retrieval -> Ranking -> Re-ranking).\n",
    "* **Layer 4 (Model Hosting & Orchestration):** Serves ML models at scale with low-latency endpoints; supports A/B testing and dynamic pipeline routing.\n",
    "* **Layer 5 (Observability):** The \"Closed-Loop\" mechanism that catches silent failures and triggers automated retraining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defea36d",
   "metadata": {},
   "source": [
    "![Meta Scale Recommender System](Meta_Scale_Recommender_System.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa17b596",
   "metadata": {},
   "source": [
    "## 2. Tiered Feature Store & State Management\n",
    "Managing \"Truth\" at scale requires three distinct storage types synchronized by a Metadata Layer (e.g., Tecton).\n",
    "\n",
    "### The Tri-Store Strategy:\n",
    "1.  **Offline Store (Snowflake/Iceberg):** Stores historical \"Point-in-Time\" features for model training.\n",
    "2.  **Online Store (Redis/DynamoDB):** Optimized for <10ms KV lookups of latest features for Ranking.\n",
    "3.  **Vector DB (FAISS/Milvus):** Stores Item Embeddings in an ANN (Approximate Nearest Neighbor) index for Retrieval.\n",
    "\n",
    "![Data_Ingestion_Model_Pipeline](Data_Ingestion_Model_Pipeline.png)\n",
    "\n",
    "### Data Flow Nuance:\n",
    "* **Multimodal Input:** Content (Video, Audio, Text) is pre-processed into embeddings and stored here.\n",
    "* **Consistency:** The Metadata Layer ensures that the features used during training match the features available at inference time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4d9f5d",
   "metadata": {},
   "source": [
    "## 3. Two-Tower Model: From Alignment to Serving\n",
    "The \"Brain\" of the retrieval stage is the Two-Tower model, which learns to map diverse users and items into a single shared vector space.\n",
    "\n",
    "\n",
    "### Phase 1: Offline Training (The Left Panel)\n",
    "* **Objective:** Align User and Item towers using **Contrastive Loss**.\n",
    "* **Inputs:** A \"Hybrid\" funnel of Sparse Metadata (IDs), Dense Metadata (Counts), and Pre-computed Content Embeddings (BERT/ResNet).\n",
    "* **Output:** Learned weights that understand high-level correlations (e.g., \"Python developers buy mechanical keyboards\").\n",
    "\n",
    "### Phase 2: Online Serving (The Right Panel)\n",
    "* **Mechanism:** Towers are \"frozen\" and separated. \n",
    "* **User Tower:** Runs as a live microservice to convert user context into a query vector $U_{live}$.\n",
    "* **Item Tower:** Run in batch to populate the **ANN Index** with $V_i$ vectors.\n",
    "\n",
    "# ![Two Tower Model Overview](Two-tower_Model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3292a808",
   "metadata": {},
   "source": [
    "## 3.1 Stage 1: Candidate Retrieval (The Broad Funnel)\n",
    "The Retrieval stage is the first filter in the recommendation funnel. Its primary job is to reduce the universe of items (Millions/Billions) down to a manageable set of high-probability candidates (~1,000) in under 50ms.\n",
    "\n",
    "### Training Strategy:\n",
    "* **Model Type:** Two-Tower (Dual Encoder) Architecture.\n",
    "* **Architecture:** Two separate neural networks (User Tower & Item Tower) that never interact until the final output layer.\n",
    "* **Loss Function:** **Contrastive Loss** or **In-batch Softmax**. It teaches the model to maximize the dot product of (User, Clicked Item) and minimize it for random negatives.\n",
    "* **Dataset:** Positive interactions (Clicks, Likes) and \"Easy Negatives\" (randomly sampled items the user has never seen).\n",
    "\n",
    "\n",
    "\n",
    "### Key Mechanism: The Shared Embedding Space\n",
    "The model maps both Users and Items into the same $N$-dimensional vector space. \n",
    "* **Items** are pre-computed and indexed in a **Vector Database (FAISS/HNSW)**.\n",
    "* **Users** are mapped into the space in real-time based on their current context.\n",
    "\n",
    "### Engineering Trade-offs:\n",
    "| Feature | Strategy | Why? |\n",
    "| :--- | :--- | :--- |\n",
    "| **Indexing** | Approximate Nearest Neighbor (ANN) | Exact KNN is $O(N)$, too slow for Meta-scale. ANN is $O(\\log N)$. |\n",
    "| **Embeddings** | Hybrid (Content + ID) | Solves the \"Cold Start\" problem for new items using BERT/ViT features. |\n",
    "| **Updating** | Asynchronous | Item index is updated hourly; User vectors are updated in real-time. |\n",
    "\n",
    "### Why we need this:\n",
    "Without Retrieval, the Ranking model would have to score every item in the database for every user request, which is computationally impossible at scale. Retrieval acts as a \"Fast & Cheap\" filter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13522de6",
   "metadata": {},
   "source": [
    "## 3.2 Stage 2: Heavy Ranking (The Precision Scorer)\n",
    "Once the Two-Tower model provides 1,000 candidates, the **Ranking Model** performs a surgical evaluation to predict the probability of engagement.\n",
    "Because the Ranker only deals with ~1,000 items (not 1 billion), we can use a Heavyweight Deep Neural Network (DNN) or a Transformer that looks at the \"cross-features\" (interactions) between the user and the item in extreme detail.\n",
    "\n",
    "### Training Strategy:\n",
    "* **Model Type:** Deep & Cross Network (DCN) or DLRM.\n",
    "* **Input:** Concatenated vector of User, Item, and Context features + Feature Interactions.\n",
    "* **Loss Function:** Binary Cross-Entropy (Log Loss).\n",
    "* **Dataset:** Uses \"Logged Impressions\" (Items shown to user) to distinguish between clicks (1) and non-clicks (0).\n",
    "\n",
    "### Key Difference from Retrieval:\n",
    "| Feature | Retrieval (Stage 1) | Ranking (Stage 2) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Input Size** | 1 Billion Items | ~1,000 Items |\n",
    "| **Model Complexity** | Simple (Two-Tower) | Complex (Deep Neural Network) |\n",
    "| **Interactions** | Simple Dot Product | Complex Cross-Layers / Transformers |\n",
    "| **Output** | Unordered Set | Sorted Probability Scores |\n",
    "\n",
    "### Why we need this:\n",
    "Retrieval is \"broad and fast\"; Ranking is \"narrow and deep.\" Ranking can afford to look at hundreds of features (like current battery life or network speed) that would be too expensive to calculate for every item in the database.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d668ec59",
   "metadata": {},
   "source": [
    "## 3.3 Stage 3: Re-Ranking (Post-Processing & Business Logic)\n",
    "The final stage of the funnel takes the top-scored items from the Ranker and applies heuristic-based rules to ensure the final list is diverse, fair, and aligned with business goals.\n",
    "\n",
    "### Key Objectives:\n",
    "* **Diversity:** Preventing \"Filter Bubbles\" by ensuring the user doesn't see 10 videos from the same creator or topic in a row.\n",
    "* **Deduplication:** Removing items the user has already seen or purchased recently (Frequency Capping).\n",
    "* **Fairness:** Ensuring \"Equalized Odds\" for content creators from underrepresented groups.\n",
    "* **Business Constraints:** Promoting \"Sponsored\" content or high-margin products while maintaining relevance.\n",
    "\n",
    "\n",
    "\n",
    "### Common Algorithms:\n",
    "1.  **Maximal Marginal Relevance (MMR):** An iterative algorithm that balances the relevance score with the \"novelty\" of an item compared to items already selected for the list.\n",
    "2.  **Determinantal Point Processes (DPP):** A sophisticated mathematical approach to modeling the \"repulsion\" between similar items to maximize global set diversity.\n",
    "\n",
    "### Why we need this:\n",
    "A \"Pure\" ML Ranker is greedy—it will only show what is most likely to be clicked. Without Re-ranking, a news feed might become repetitive (the same viral video over and over), or biased, leading to long-term user fatigue and churn.\n",
    "\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1a6083",
   "metadata": {},
   "source": [
    "## Layer 4: Production Serving & Telemetry Collection\n",
    "This layer is responsible for the elastic scaling of model inference and the high-fidelity capture of \"Ground Truth\" data required for observability.\n",
    "\n",
    "### 4.1 Serving Infrastructure (KServe / SageMaker)\n",
    "To handle Meta-scale traffic (millions of queries per second), we deploy models as distributed microservices.\n",
    "* **Elastic Scaling:** Using Kubernetes (KServe) to scale pods based on CPU/GPU utilization or request latency (P99).\n",
    "* **Model Versioning:** Supporting A/B testing or Canary deployments where 5% of traffic is routed to a new model version to compare performance.\n",
    "\n",
    "\n",
    "\n",
    "### 4.2 Telemetry Collector (The \"Truth\" Capture)\n",
    "We cannot measure model success without capturing what the user actually did.\n",
    "* **Event Capture:** Using **OpenTelemetry** or **Fluentd** to log the \"Inference Trio\":\n",
    "    1. **The Features:** What we thought we knew about the user.\n",
    "    2. **The Prediction:** What the model predicted (e.g., 0.85 CTR).\n",
    "    3. **The Outcome:** Did the user actually click? (The Ground Truth).\n",
    "* **Stream Joiner:** A Flink job joins these asynchronous logs back together by `Request_ID` to create the training dataset for the next model iteration.\n",
    "\n",
    "### Key Performance Indicators (KPIs):\n",
    "| Metric | Purpose | Target |\n",
    "| :--- | :--- | :--- |\n",
    "| **P99 Latency** | User Experience | < 200ms (End-to-End) |\n",
    "| **Throughput** | Cost Efficiency | 100k+ RPS |\n",
    "| **Log Loss** | Model Accuracy | Minimizing divergence between Prediction and Outcome |\n",
    "\n",
    "### Why we need this:\n",
    "Without a robust Telemetry layer, the system is \"blind.\" We would have no way to know if our models are actually performing in the real world or if we are serving irrelevant content to our users.\n",
    "\n",
    "![Serving and Telemetry Layer](Serving%20and%20Telemetry%20Layer.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b0e636",
   "metadata": {},
   "source": [
    "## 5. Layer 5: ML Observability & Automated Remediation (The Feedback Loop)\n",
    "This layer acts as the system's \"Immune System.\" Its goal is to detect **Model Decay** (performance dropping over time) and **Data Drift** (user behavior changing) before they impact business revenue.\n",
    "\n",
    "### 5.1 The Observability Stack (Not just DevOps)\n",
    "Unlike standard monitoring (latency/errors), ML Observability requires statistical analysis of the data payload.\n",
    "\n",
    "* **Drift Detection Engine:**\n",
    "    * **Input:** Compares *Reference Data* (Training Set) vs. *Current Inference Data* (Live Stream).\n",
    "    * **Algorithms:**\n",
    "        * **PSI (Population Stability Index):** For categorical shifts (e.g., \"Why are we suddenly getting 50% more users from Brazil?\").\n",
    "        * **KS-Test (Kolmogorov-Smirnov):** For numerical shifts (e.g., \"The average 'Time Spent' feature dropped from 60s to 5s\").\n",
    "\n",
    "### 5.2 The \"Self-Healing\" Controller\n",
    "This is the automated logic that closes the loop, turning a linear pipeline into a cycle.\n",
    "\n",
    "1.  **Detection:** Monitor alerts that `Drift_Score > 0.15` (Significant Drift).\n",
    "2.  **Decision:** Controller Logic evaluates the severity.\n",
    "3.  **Remediation (The \"Webhook\"):**\n",
    "    * **Scenario A (Data Issue):** If input data is corrupted (lots of Nulls), trigger a \"Circuit Breaker\" to rollback to the previous stable model version.\n",
    "    * **Scenario B (Natural Drift):** If user interest has shifted (e.g., new viral trend), trigger the **Airflow Pipeline** to start a \"Warm Start\" retraining job using the freshest data.\n",
    "\n",
    "\n",
    "### Why this matters:\n",
    "Without Layer 5, a recommendation engine is a \"melting ice cube.\" It is smartest the day it is deployed and gets dumber every hour. Layer 5 ensures the model continuously adapts to the world.\n",
    "\n",
    "![Layer 5 Observability & Self-Healing Diagram](Observability%20and%20AutomatedRemedy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b42fec8",
   "metadata": {},
   "source": [
    "# Start of Interview "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a833b207",
   "metadata": {},
   "source": [
    "# ML SYSTEM DESIGN — 1 HOUR MOCK INTERVIEW (FAANG)\n",
    "**Level:** Senior → Staff ML Engineer  \n",
    "**System:** Large-scale Recommendation System  \n",
    "**Audience:** Meta / Google / Amazon  \n",
    "**Purpose:** Deep interview rehearsal (no gaps)\n",
    "\n",
    "---\n",
    "\n",
    "## INTERVIEW STRUCTURE (60 MIN)\n",
    "\n",
    "- 0–8 min   → Problem framing & clarifications\n",
    "- 8–15 min  → Data & learning signals\n",
    "- 15–25 min → Feature & representation layer\n",
    "- 25–40 min → Modeling & decision pipeline\n",
    "- 40–48 min → Serving, scaling & latency\n",
    "- 48–55 min → Evaluation, experimentation & ops\n",
    "- 55–60 min → Ethics, risk & failure modes\n",
    "\n",
    "---\n",
    "\n",
    "# 0. PROBLEM FRAMING & CLARIFICATIONS (0–8 MIN)\n",
    "\n",
    "## Q1. What clarifying questions do you ask before designing?\n",
    "**Ideal Answer:**\n",
    "Before proposing any architecture, I clarify the problem space to avoid optimizing the wrong objective.\n",
    "\n",
    "I would ask:\n",
    "- What is the primary product goal: short-term engagement (CTR) or long-term value (retention, satisfaction)?\n",
    "- What entities are we recommending? (posts, videos, ads, products)\n",
    "- What content modalities are involved (text, video, image, mixed)?\n",
    "- What are the latency requirements (P50 / P99)?\n",
    "- Is personalization per-user, per-session, or contextual?\n",
    "- Are there safety, policy, or fairness constraints?\n",
    "\n",
    "This ensures system design choices align with product intent and constraints.\n",
    "\n",
    "---\n",
    "\n",
    "## Q2. How do you define success for this system?\n",
    "**Ideal Answer:**\n",
    "Success is defined using **long-term metrics**, not just immediate engagement.\n",
    "\n",
    "Primary metrics:\n",
    "- Retention (D1, D7, D30)\n",
    "- Session depth and duration\n",
    "- Repeat usage\n",
    "\n",
    "Secondary metrics:\n",
    "- CTR\n",
    "- Likes / follows\n",
    "\n",
    "I explicitly avoid over-optimizing CTR alone, as it can encourage clickbait and harm long-term trust.\n",
    "\n",
    "---\n",
    "\n",
    "## Q3. How do product goals influence technical decisions?\n",
    "**Ideal Answer:**\n",
    "Product goals directly influence:\n",
    "- Loss functions (ranking vs classification)\n",
    "- Exploration vs exploitation balance\n",
    "- Model complexity vs latency\n",
    "- Diversity constraints\n",
    "\n",
    "For example, a retention-focused system prioritizes diversity and novelty, while a revenue-focused system prioritizes precision and conversion.\n",
    "\n",
    "---\n",
    "\n",
    "# 1. DATA & LEARNING SIGNALS (8–15 MIN)\n",
    "\n",
    "## Q4. What data sources power the recommendation system?\n",
    "**Ideal Answer:**\n",
    "The system relies primarily on **implicit feedback**, including:\n",
    "- Views\n",
    "- Clicks\n",
    "- Dwell time\n",
    "- Scroll behavior\n",
    "\n",
    "Additional signals include:\n",
    "- Explicit feedback (likes, follows)\n",
    "- Content metadata\n",
    "- Context (time, device, location)\n",
    "\n",
    "At scale, implicit feedback dominates because explicit feedback is sparse.\n",
    "\n",
    "---\n",
    "\n",
    "## Q5. How do offline and online signals differ?\n",
    "**Ideal Answer:**\n",
    "Offline signals:\n",
    "- Historical logs\n",
    "- Used for training and evaluation\n",
    "- Stable but delayed\n",
    "\n",
    "Online signals:\n",
    "- Session-level interactions\n",
    "- Used for real-time adaptation\n",
    "- Noisy but timely\n",
    "\n",
    "Offline learning builds general preference models; online signals personalize in-session behavior.\n",
    "\n",
    "---\n",
    "\n",
    "## Q6. How do feedback loops affect recommendations?\n",
    "**Ideal Answer:**\n",
    "Feedback loops occur when recommendations influence what users see, biasing future data.\n",
    "\n",
    "Mitigation strategies:\n",
    "- Randomized exploration\n",
    "- Injecting diversity\n",
    "- Counterfactual evaluation\n",
    "- Periodic retraining with corrected labels\n",
    "\n",
    "Ignoring feedback loops leads to echo chambers and bias amplification.\n",
    "\n",
    "---\n",
    "\n",
    "## Q7. How do you detect data drift?\n",
    "**Ideal Answer:**\n",
    "I monitor:\n",
    "- Feature distribution shifts\n",
    "- Prediction confidence changes\n",
    "- Training vs serving feature statistics\n",
    "- Sudden drops in online metrics\n",
    "\n",
    "Drift detection triggers investigation, not automatic retraining.\n",
    "\n",
    "---\n",
    "\n",
    "# 2. FEATURE & REPRESENTATION LAYER (15–25 MIN)\n",
    "\n",
    "## Q8. What is the difference between data engineering and feature engineering?\n",
    "**Ideal Answer:**\n",
    "- Data engineering focuses on ingestion, cleaning, and storage.\n",
    "- Feature engineering focuses on transforming clean data into stable, reusable, model-consumable signals.\n",
    "\n",
    "Feature engineering emphasizes:\n",
    "- Semantic meaning\n",
    "- Temporal stability\n",
    "- Reusability across models\n",
    "\n",
    "---\n",
    "\n",
    "## Q9. What user features would you create?\n",
    "**Ideal Answer:**\n",
    "User features include:\n",
    "- Long-term interest embeddings\n",
    "- Recency-weighted interaction aggregates\n",
    "- Topic affinity distributions\n",
    "- Session-level context features\n",
    "\n",
    "These features capture both stable preferences and short-term intent.\n",
    "\n",
    "---\n",
    "\n",
    "## Q10. What item/content features are important?\n",
    "**Ideal Answer:**\n",
    "Item features include:\n",
    "- Content embeddings\n",
    "- Popularity trends\n",
    "- Freshness signals\n",
    "- Metadata (creator, category)\n",
    "\n",
    "Embeddings enable semantic matching even with sparse interaction data.\n",
    "\n",
    "---\n",
    "\n",
    "## Q11. What is a feature store and why is it critical?\n",
    "**Ideal Answer:**\n",
    "A feature store provides:\n",
    "- Centralized feature definitions\n",
    "- Offline/online consistency\n",
    "- Low-latency access\n",
    "\n",
    "It prevents training–serving skew and ensures reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "## Q12. How do you handle cold start for users?\n",
    "**Ideal Answer:**\n",
    "Cold start strategies include:\n",
    "- Asking for explicit preferences\n",
    "- Using demographic or contextual priors\n",
    "- Popularity-based recommendations\n",
    "- Rapid exploration\n",
    "\n",
    "As soon as interactions occur, personalization ramps up quickly.\n",
    "\n",
    "---\n",
    "\n",
    "## Q13. How do you handle cold start for items?\n",
    "**Ideal Answer:**\n",
    "For new items:\n",
    "- Generate content-based embeddings\n",
    "- Use metadata priors\n",
    "- Allocate exploration traffic\n",
    "- Monitor early engagement signals\n",
    "\n",
    "This avoids starving new content.\n",
    "\n",
    "---\n",
    "\n",
    "# 3. MODELING & DECISION PIPELINE (25–40 MIN)\n",
    "\n",
    "## Q14. Why use a multi-stage recommendation architecture?\n",
    "**Ideal Answer:**\n",
    "A single model cannot scale across billions of items.\n",
    "\n",
    "Multi-stage architecture:\n",
    "- Candidate generation → maximize recall\n",
    "- Ranking → maximize precision\n",
    "- Re-ranking → enforce constraints\n",
    "\n",
    "Each stage optimizes a different objective under different latency constraints.\n",
    "\n",
    "---\n",
    "\n",
    "## Q15. How does candidate generation work?\n",
    "**Ideal Answer:**\n",
    "Candidate generation uses:\n",
    "- Jointly trained user and item embeddings\n",
    "- ANN search to retrieve top-N items\n",
    "- Emphasis on speed and recall\n",
    "\n",
    "If a relevant item is missed here, downstream models cannot recover it.\n",
    "\n",
    "---\n",
    "\n",
    "## Q16. Why does user–item similarity work?\n",
    "**Ideal Answer:**\n",
    "User and item embeddings are trained in a shared space using interaction-based objectives.\n",
    "\n",
    "The dot product approximates engagement likelihood because training aligns users with items they interact with.\n",
    "\n",
    "---\n",
    "\n",
    "## Q17. What models do you use for ranking?\n",
    "**Ideal Answer:**\n",
    "Ranking models depend on constraints:\n",
    "- GBDT for low latency and interpretability\n",
    "- Neural networks for richer interactions\n",
    "- Transformers for sequential personalization\n",
    "\n",
    "Choice balances accuracy, latency, and operational complexity.\n",
    "\n",
    "---\n",
    "\n",
    "## Q18. How do you incorporate user history?\n",
    "**Ideal Answer:**\n",
    "Approaches include:\n",
    "- Aggregated behavioral features\n",
    "- Sequential encoders (RNNs / Transformers)\n",
    "- Cross-attention between history and candidates\n",
    "\n",
    "Sequential models capture evolving intent.\n",
    "\n",
    "---\n",
    "\n",
    "## Q19. Which loss functions are appropriate?\n",
    "**Ideal Answer:**\n",
    "Ranking requires ranking-aware losses:\n",
    "- Pairwise loss (e.g., hinge)\n",
    "- Listwise loss (softmax over slate)\n",
    "\n",
    "Classification loss ignores relative ordering and is suboptimal.\n",
    "\n",
    "---\n",
    "\n",
    "## Q20. Can recommendations adapt within a session?\n",
    "**Ideal Answer:**\n",
    "Yes, using:\n",
    "- Lightweight re-ranking\n",
    "- Session embeddings\n",
    "- Fast feature updates\n",
    "\n",
    "Heavy models remain offline; session adaptation is constrained by latency.\n",
    "\n",
    "---\n",
    "\n",
    "# 4. SERVING, SCALING & LATENCY (40–48 MIN)\n",
    "\n",
    "## Q21. How do you serve recommendations at scale?\n",
    "**Ideal Answer:**\n",
    "- Stateless services\n",
    "- Horizontal scaling\n",
    "- Precomputed embeddings\n",
    "- Aggressive caching\n",
    "\n",
    "Statelessness enables reliability and elasticity.\n",
    "\n",
    "---\n",
    "\n",
    "## Q22. What is a typical latency budget?\n",
    "**Ideal Answer:**\n",
    "P99 latency is usually <200ms, allocated across:\n",
    "- Candidate retrieval\n",
    "- Ranking\n",
    "- Re-ranking\n",
    "- Network overhead\n",
    "\n",
    "Latency budgets are explicitly enforced.\n",
    "\n",
    "---\n",
    "\n",
    "## Q23. How do you handle traffic spikes?\n",
    "**Ideal Answer:**\n",
    "- Autoscaling\n",
    "- Cached fallbacks\n",
    "- Graceful degradation\n",
    "\n",
    "If ranking fails, serve popularity-based results.\n",
    "\n",
    "---\n",
    "\n",
    "# 5. EVALUATION, EXPERIMENTATION & OPS (48–55 MIN)\n",
    "\n",
    "## Q24. What offline metrics do you use?\n",
    "**Ideal Answer:**\n",
    "- NDCG\n",
    "- MAP\n",
    "- Recall@K\n",
    "\n",
    "Offline metrics validate ranking quality but cannot predict user satisfaction alone.\n",
    "\n",
    "---\n",
    "\n",
    "## Q25. What online metrics matter?\n",
    "**Ideal Answer:**\n",
    "- Retention\n",
    "- Session length\n",
    "- Revenue (if applicable)\n",
    "\n",
    "Online metrics reflect real-world impact.\n",
    "\n",
    "---\n",
    "\n",
    "## Q26. How do you run A/B tests?\n",
    "**Ideal Answer:**\n",
    "- Randomized user assignment\n",
    "- Guardrail metrics\n",
    "- Statistical significance checks\n",
    "\n",
    "Experiments must be isolated and repeatable.\n",
    "\n",
    "---\n",
    "\n",
    "## Q27. How do you monitor models in production?\n",
    "**Ideal Answer:**\n",
    "- Prediction distributions\n",
    "- Feature drift\n",
    "- Business KPIs\n",
    "\n",
    "Alerts trigger investigation, not automatic changes.\n",
    "\n",
    "---\n",
    "\n",
    "## Q28. How do you retrain and roll back models?\n",
    "**Ideal Answer:**\n",
    "- Scheduled retraining\n",
    "- Drift-triggered retraining\n",
    "- Canary deployments\n",
    "- Instant rollback on regression\n",
    "\n",
    "Model registries ensure reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "# 6. ETHICS, RISK & FAILURE MODES (55–60 MIN)\n",
    "\n",
    "## Q29. What are major failure modes?\n",
    "**Ideal Answer:**\n",
    "- Filter bubbles\n",
    "- Bias amplification\n",
    "- Harmful or misleading content\n",
    "\n",
    "These risks grow with scale.\n",
    "\n",
    "---\n",
    "\n",
    "## Q30. How do you address fairness?\n",
    "**Ideal Answer:**\n",
    "- Bias audits\n",
    "- Fairness-aware evaluation\n",
    "- Controlled exposure\n",
    "\n",
    "Fairness is monitored, not assumed.\n",
    "\n",
    "---\n",
    "\n",
    "## Q31. How do you explain recommendations?\n",
    "**Ideal Answer:**\n",
    "- High-level explanations for users\n",
    "- Detailed diagnostics internally\n",
    "\n",
    "Explainability builds trust and aids debugging.\n",
    "\n",
    "---\n",
    "\n",
    "## END OF INTERVIEW\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ldm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
